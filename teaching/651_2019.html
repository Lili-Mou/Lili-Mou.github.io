<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"></head><body><h1>CMPUT 651: Topics in Artificial Intelligence</h1>
<h2>Deep Learning for NLP</h2>
<h3>Fall 2019</h3>
<h3>Instructor: Lili Mou</h3>

<br />
	
<h3>Course Description</h3>
This course introduces deep learning (DL) techniques for natural language processing (NLP).  <br /> 
Contrary to other DL4NLP courses, we would have a whirlwind  tour of
 all neural<br /> architectures (e.g., CNNs, RNNs, attention) in a few lectures. Then, we would make<br /> 
  significant efforts in learning structured prediction using Bayesian and Markov networks,<br />
  with applications of sequential labeling, syntactic parsing, and sentence generation. In this<br /> 
   process, we will also see  how such traditional methods can be combined with and improve<br /> a plain neural network.<br />
 <br />
<br />
	
<h3>Prerequisites</h3>
<ul>
	<li>Basic math (including algebra, calculus, probability theory, and statistics)</li>
	<li>Background machine learning knowledge (e.g., logistic regression, softmax classification)</li>
	<li>Coding skills. The students should be able to write code in at least one programming <br/>
		language, although the course project would be in python by default. The students should<br/>
		also be able to implement  algorithms by themselves, as well as making use of existing <br/>
	   packages	(such as TensorFlow and PyTorch).</li>
</ul>
<b>No</b> DL or NLP background is required. They will be self-contained.

<br/><br />
<h3>Tentative Syllabus</h3>
<ol>
<b><li>Neural network basics</li></b>
	<ul>
		<li>Classification tasks and classifiers</li>
						<ul>
						<li>Naive Bayes, logistic regression, softmax, etc.</li>
					</ul>
		<li>Deep Neural Networks</li>
						<ul>
									<li>Forward and backward propagation</li>
						</ul>
		<li>Embeddings: Representing Discrete Words</li>
		<li>Representing Structured Input</li>
					<ul>
							<li>CNNs, RNNs, attention</li>
					</ul>
	</ul>
<b><li>Structured Prediction</li></b>
	<ul>
	<li>Bayesian Networks</li>
						<ul>
		<li>HMM for sequential labeling</li>
			</ul>
	<li>Markov Networks & Conditional Random Fields</li>
	<li>Syntactic Parsing</li>
	</ul>
	</ul>
<b><li>Sentence Generation</li></b>
			<ul>
					 <li>Variational Autoencoder</li>
					<li>Sampling and Stochastic Searching</li>
			</ul>

<b><li>Discrete Latent Space</li></b>
					<ul>
			<li>Reinforcement Learning in NLP</li>
			<li>Neural Relaxation for RL</li>
		</ul>
</ol>
The syllabus is subject to changes.

<h3>Lectures</h3>

01. NLP Tasks and Linear Classification [<a href="https://drive.google.com/open?id=1Fk5Z_871k1X5T19QXH2jJLnNxZialdMS" target="_blank">slides</a>] 
<br />
	
02. Deep Neural Network [<a href="https://drive.google.com/file/d/12sBfi3i_OPq5YIlBKTkOJpIAX1d5YUf9/view?usp=sharing" target="_blank">slides</a>] 
<br/>
	
03. Word Embeddings and Language Modeling [<a href="https://drive.google.com/file/d/1ActfCHoHsdq9h9cyt_cxyEWZQ1DbVf66/view?usp=sharing" target="_blank">slides</a>] 
<br/>

04. CNNs, RNNs, etc. [<a href="https://drive.google.com/file/d/1SvhceV2QRcp8faXKeRmm0SXPo-dhKktf/view" target="_blank">slides</a>]
<br/>
	
05. Seq2Seq Models and Attention Mechanism [<a href="https://drive.google.com/file/d/1hJRuOmIg5UuLpoqBqfq9ymviYBVcDDcB/view?usp=sharing" target="_blank">slides</a>]  
	
<br/><br /><br/><br /><br/><br /><br/><br /><br/><br /><br/><br /><br/><br /><br/><br />