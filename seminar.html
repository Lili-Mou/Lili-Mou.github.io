
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset=utf-8">
	<TITLE></TITLE>
	<META NAME="GENERATOR" CONTENT="LibreOffice 4.1.2.3 (Linux)">
	<META NAME="CREATED" CONTENT="0;0">
	<META NAME="CHANGED" CONTENT="20160324;53506310905905">
	<STYLE TYPE="text/css">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=8">
<meta http-equiv="Expires" content="0">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Cache-control" content="no-cache">
<meta http-equiv="Cache" content="no-cache">
	<!--
		H3.ctl { font-family: "Lohit Hindi" }
	-->
	</STYLE>
</HEAD>
<BODY LANG="en-US" DIR="LTR">
<H3 CLASS="western">Notes</H3>
<P><A HREF="resource/backprop.pdf" TARGET="_blank">Cheatsheet for Backpropagation</A></P>
<P><A HREF="resource/MarkovNet.pdf" TARGET="_blank">Notes on Markov
Networks</A></P>
<P><A HREF="resource/RL.pdf" TARGET="_blank">Mind Map of Reinforcement Learning</A></P>

<H3 CLASS="western">Seminars</H3>

<li> 2010--, Software Institute, Peking University</li>
<li> 2013--2015, SIG ML/NLP, Software Institute, Peking University</li>
<li> 2015.11--, Baidu Inc. with Rui Yan</li>

<br />

<p>
Dec 2016/Jun 2017. Special Talks on Scientific Writing [<a href="resource/writing.pdf">slides</a>]
</p>
<TABLE CELLPADDING=2 CELLSPACING=2>
<TR>
			<TD WIDTH=76 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>27 Mar 2017</P>
		</TD>
		<TD WIDTH=148 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><a href="resource/neural_programming_0.pdf" target="blank">Neural Programming</a><br />
		<TD WIDTH=456 STYLE="border: none; padding: 0in">
			<P></P>
	</TR>


		<TR>
			<TD WIDTH=76 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>11 May/08 Jun</P>
		</TD>
		<TD WIDTH=148 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>Sequence Generation<br /> [<a href="resource/gen1.pdf" target="_blank">1</a>,
			<a href="resource/gen2.pdf" target="_blank">2</a>]</P>
		</TD>
		<TD WIDTH=456 STYLE="border: none; padding: 0in">
			<P>Deep learning is far beyond CNNs, RNNs, etc. In these two seminars, Yunchuan and I introduced several recent techniques of sequence (sentence) generation, including sampling approaches, reinforcement learning, and variational autoencoding.</P>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>

	<TR>
			<TD WIDTH=76 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>23/27 Apr</P>
		</TD>
		<TD WIDTH=148 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><a href="resource/KB.pdf" target="_blank">Trans*</a></P>
		</TD>
		<TD WIDTH=456 STYLE="border: none; padding: 0in">
			<P>Trans* is a family method of learning the vector representations of entities and 
			relations in a knowledge base (or a knowledge graph---Don't ask me the difference). 
            From |<b><i>h</i></b>+<b><i>t</i></b>-<b><i>r</i></b>| started all.</P>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>

	
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=76 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>04 Apr 2016</P>
		</TD>
		<TD WIDTH=148 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><a href="AlphaGo.pdf" target="_blank">AlphaGo</a></P>
		</TD>
		<TD WIDTH=456 STYLE="border: none; padding: 0in">
			<P>(Courtesy of Yunchuan) A combination of convolutional neural network (CNN) and Monta Carto tree search (MCTS).</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=76 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>27 Mar 2016</P>
		</TD>
		<TD WIDTH=148 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><a href="resource/symbolics.pdf" target="_blank">Neural Symbolics</a></P>
		</TD>
		<TD WIDTH=456 STYLE="border: none; padding: 0in">
			<P STYLE="page-break-before: always">A series work from Noah's Ark
			Lab, Huawei. My understanding is to design a (complicated) neural
			network to mimic human behaviors: modeling a sentence, querying a
			table/KB, selecting a field/column, selecting a row, copying
			something, etc. Several challenges of end-to-end neuralized
			learning include differentiability, supervision, scalability.</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=76 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>26 Mar 2016</P>
		</TD>
		<TD WIDTH=148 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><a href="Alzheimer.pdf" target="_blank">Neural science and Alzheimer's disease</P>
		</TD>
		<TD WIDTH=456 STYLE="border: none; padding: 0in">
			<P>(Courtesy of Yu Wu) Ankyrin G (AnkG) plays a critical role at the axon initial segment (AIS). AnkG downregulartion induces impaired selective filtering machineary at AIS. Impaired AIS filtering might underlie functional defects in APP/PS1 neurons. <FONT COLOR="#0000ff">Disclaimer</FONT>: I am not an expert in neural science.</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>


	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>08 Mar 2016</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/GAN.pdf" target="_blank">Generative Adversarial Nets </A>
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>A combition of neural networks and game theory. Imagine that we
			have two agents <B>G</B>enerator and <B>D</B>iscriminator: <B>G</B>
			generates fake samples, while <B>D</B> tries to distinguish these
			fake samples in disguise. The objective is to minimize<SUB>G</SUB>
			max<SUB>D</SUB> V(D,G). 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>28 Oct 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>Variational Autoencoders 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>(By Yunchuan) Variational autoencoders give a distribution of
			hidden variables, <B>z</B>, while traditional autoencoders compute
			<B>z</B> in a deterministic way. But why is it useful in practice?
						</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>28 Oct 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/adaptation.pdf" TARGET="_blank">Domain
			Adaptation</A> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>Including EasyAdapt, instance weighting, and structural
			correspondence learning. I am, in fact, curious about adaptation
			in neural network-based settings. However, NNs are adaptable by
			the incremental/multi-task training nature. Therefore, there is
			little point, as far as I can currently see, in NN adaptation.
			<FONT COLOR="#0000ff">Nevertheless, I have conducted a series of
			comparative studies to shed more light on transferring knowledge in neural networks [<A HREF="http://arxiv.org/pdf/1603.06111.pdf" TARGET="_blank">pdf</A> (EMNLP-16)]. 
</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>21 Oct 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/variational.pdf" TARGET="_blank">Variational
			Inference (again)</A> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>Let <B>x</B> be visible variables, and <B>z</B> be invisible
			(hidden) ones. Estimating p(<B>x</B>) is usually difficult because
			we have to sum/integrate over <B>z</B>. A variational lower bound
			peaks when <B>z</B>~p(<B>z</B>|<B>x</B>), which is oftentimes
			intractable. The mixture of Gaussian, for example, assumes <B>z</B>
			in parametric forms, i.e., Gaussian. In VI in general, we still
			have to restrict the form of <B>z</B>, but not in a parametric
			way. A typical approximation is factorization, that it, p(<B>z</B>)=|&nbsp;|<SUB>i</SUB>
			p(<B>z</B><SUB>i</SUB>). 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>14 Oct 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>Attention-based Networks 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>(By Hao Peng) The encoding-decoding model opens a new era of
			sequence generation. It is unrealistic, however, to encode a very
			long input sequence to a fixed vector. The attention mechanism is
			designed to aggregate information over the input sequence by an
			adaptable weighted sum. Selected Papers: <A HREF="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" TARGET="_blank">NIPS'14,
			pp. 3104--3112</A>, <A HREF="http://arxiv.org/abs/1409.0473" TARGET="_blank">ICLR'15</A>,
			<A HREF="http://jmlr.org/proceedings/papers/v37/xuc15.pdf" TARGET="_blank">ICML'15</A>
			<A HREF="http://www.anthology.aclweb.org/D/D15/D15-1044.pdf" TARGET="_blank">EMNLP'15,
			pp. 319--389</A> <A HREF="http://www.anthology.aclweb.org/D/D15/D15-1166.pdf" TARGET="_blank">EMNLP'15,
			pp. 1412--1421</A> 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>14 Oct 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/PCFG.pdf" TARGET="_blank">Discourse
			Parsing with PCFG</A> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>We wrap up discourse analysis by PCFG-based discourse parsing,
			which requires probabilistic context-free grammar in general. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>23 Sep 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/discourse.pdf" TARGET="_blank">Discourse
			Analysis</A> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>We shall also explore various NLP research topics, and
			discourse analysis, discussed in this seminar, precedes our
			horizon expansion. Notice that the slide is nothing but snapshots
			of papers in the proceedings, and in fact has little substance. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>22 Jul 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/variational.pdf" TARGET="_blank">Variational
			Inference</A> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>I am a tyro in variational inference. Please refer to Ch 10,
			<I>Pattern Recognition and Machine Learning</I>. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD COLSPAN=3 STYLE="border: none; padding: 0in">
			<P><FONT COLOR="#0000ff">Bad news:</FONT> Thursday evening's
			seminars are suspended temporarily.<BR><FONT COLOR="#ff0000">Good
			news:</FONT> I am reading <I>Statistical Decision Theory and
			Bayesian Analysis</I>, by James O. Berger (1985). Following list
			some hopefully useful materials. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 STYLE="border: none; padding: 0in">
			<P>Ch 1: Losses, Risks and Decision Principles</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in">
			<P>&nbsp;</P>
		</TD>
		<TD VALIGN=TOP STYLE="border: none; padding: 0in">
			<P ALIGN=RIGHT>Resources:&nbsp;</P>
		</TD>
		<TD STYLE="border: none; padding: 0in">
			<P><A HREF="resource/SDTBA_digest_1.pdf" TARGET="_blank">My
			textual digest</A>, highlighting some meaningful philosophy
			discussion in the textbook.<BR><A HREF="resource/SDTBA_CH1.pdf" TARGET="_blank">My
			written note</A>, mostly derived from the textbook with
			remarks.<BR><A HREF="http://icl.pku.edu.cn/member/yujs/papers/pdf/BasicConcept.pdf" target="_blank">Slide</A>,
			by Dr. Yu, who was the instructor of my undergraduate course
			<I>Probability Theory and Statistics</I>. I was always agitated
			after his lectures. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 STYLE="border: none; padding: 0in">
			<P>Ch 2: Utilities and Losses [<A HREF="resource/SDTBA_digest_2.pdf" TARGET="_blank">digest</A>,
			<A HREF="resource/SDTBA_CH2.pdf" TARGET="_blank">note</A>,
			<A HREF="http://icl.pku.edu.cn/member/yujs/papers/pdf/Utility.pdf" TARGET="_blank">slide</A>
			by Dr. Yu]</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 STYLE="border: none; padding: 0in">
			<P>Ch 3: Prior Information and Subjective Probability [<A HREF="resource/SDTBA_digest_3.pdf" TARGET="_blank">digest</A>,
			<A HREF="resource/SDTBA_CH3.pdf" TARGET="_blank">note</A>,
			<A HREF="http://icl.pku.edu.cn/member/yujs/papers/pdf/PRIOR.pdf" TARGET="_blank">slide</A>
			by Dr. Yu]</P>
		</TD>
	</TR>
<TR><TD></TD><TD COLSPAN=2><a href="resource/freqVSbayes.pdf" target="_blank">Frequentist vs Bayesian<TD></TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"> </TD>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>30 Apr 2015</P>
		</TD>
		<TD COLSPAN=2 STYLE="border: none; padding: 0in">
			<P>1. (By Yangyang Lu) A guided tour to selected papers.<BR>2.
			<A HREF="resource/GP_classification.pdf" TARGET="_blank">Gaussian
			processes for classification</A>. Ref: Ch. 6.4.5, 6.4.6, <I>Pattern
			Recognition and Machine Learning</I>. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD STYLE="border: none; padding: 0in">
			<P>Equipped with Bayesian logistic regression and GP in general,
			we find GP classification is easy except the seemingly
			overwhelming formulas.</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>29 Apr 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/Dicing.pdf" TARGET="_blank">Sampling
			methods</A> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>(Courtesy of Yunchuan Chen) God does not play dices, but we
			human do. As inference in many machine learning models is
			intractable, we have to resort to some approximations, among which
			are sampling methods. The idea of sampling is straightforward---if
			we want to estimante p(Head) of a coin, one approach is to go
			through all mathematical and physical details, which does not seem
			to be a good idea; an alternative is to toss the coin multiple
			times, giving a fairly good estimation of p(Head). However, how to
			design efficient sampling algorithms is a $64,000,000 question. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>23 Apr 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/linearClassification.pdf" TARGET="_blank">Linear
			Classification</A> <BR>Ref: Ch. 4, Ch. 6.4, <I>Pattern Recognition
			and Machine Learning</I> 
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>We first wrap up our discussion of <A HREF="resource/GP.pdf" TARGET="_blank">Gaussian
			processes</A> by introducing hyperparameter learning in kernels.
			Then we introduce linear classification models, including
			discriminant functions, probabilistic generative/ discriminative
			models, and Bayesian logistic regression (with special interest).
			Linear classification is easy---my good old friend, logistic
			regression, always serves as a baseline method in various
			applications. Through a systematic study, however, we can grasp
			the main idea behind a range of machine learning techniques. This
			seminar also precedes our future discussion on GP classfication. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>17 Apr 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>Sum Product Networks</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>(By Weizhuo Li) On some theortical aspects of SPNs, e.g.,
			normalizing, decompositionality, etc. Weizhuo also highlighted a
			'11 NIPS paper on deep architectures vis-a-vis shallow ones.</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>16 Apr 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/MemNN.pdf" TARGET="_blank">Memory
			Networks</A></P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>(Courtesy of Yangyang Lu)</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>09 Apr 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/GP.pdf" TARGET="_blank">Gaussian
			Processes</A><BR>+Bayesian linear regression</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>In this seminar, we introduce Gaussian process regression,
			which extends Bayesian linear regression with kernels. However, as
			far as I am concerned, the two models are not equivalent, even
			with finite basis functions. If I were wrong, please feel free to
			tell me.<BR><FONT COLOR="#0000ff">It was really an awesome
			seminar, filled with a whole bunch of food, drinks, and also
			fruitful discussion. [See photos <A HREF="resource/p1.jpg" TARGET="_blank">1</A>
			<A HREF="resource/p2.jpg" TARGET="_blank">2</A>
			<A HREF="resource/p3.jpg" TARGET="_blank">3</A>.]</FONT>
						</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD WIDTH=120 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>14 Jan 2015</P>
		</TD>
		<TD WIDTH=200 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/spn.ppt" TARGET="_blank">Sum
			Product Networks</A></P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>(Courtesy of Weizhuo Li) Sum product networks (SPNs) are a way
			of decomposition joint distributions. Most inference is tractable
			w.r.t. the size the the SPN network. However, it seems that
			graphical models, if converted to SPNs, have exponential numbers
			of nodes in SPNs. The story confirms the &quot;no free lunch
			theorem.&quot; As in general no perfect &quot;I-map&quot; exists
			for most real-world applications, what we have to do is to capture
			important aspects by ignoring unimportant ones. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>7 Jan 2015</P>
		</TD>
		<TD VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/DBN.pdf" TARGET="_blank">Deep
			Belief Nets </A>
			</P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>One of the most core concepts in deep learning is that &quot;do
			things wrongly and hope they work.&quot; G. Hinton introduced CD-k
			algorithm for fast training restricted Boltzmann machines; he also
			introduced layer-wise RBM pretraining for neural networks, opening
			an era of deep learning. 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
	<TR>
		<TD VALIGN=TOP STYLE="border: none; padding: 0in">
			<P>19 Dec 2014</P>
		</TD>
		<TD WIDTH=100 VALIGN=TOP STYLE="border: none; padding: 0in">
			<P><A HREF="resource/copula.pdf" TARGET="_blank">Copulas
			</A><BR>Ref: Ch. 4.6, <I>Statistical Pattern Recognition</I></P>
		</TD>
		<TD WIDTH=900 STYLE="border: none; padding: 0in">
			<P>Given marginal distributions, the joint distribution in not
			unique because of all possible kinds of independencies among
			varibles. A <I>copula</I> is defined as the joint distribution on
			a unit cube with uniform marginals. It can (just can) capture
			nontrivial independencies and link marginals with joint
			distributions. Sklar's theorem says, Copula(Marginals)=Joint 
			</P>
		</TD>
	</TR>
	<TR>
		<TD STYLE="border: none; padding: 0in"></TD>
		<TD COLSPAN=2 VALIGN=TOP STYLE="border: none; padding: 0in"></TD>
	</TR>
</TABLE>
<P><BR><BR>
</P>
</BODY>
</HTML>

